## AdaSmooth

**AdaSmooth** — это метод стохастической оптимизации, который позволяет использовать метод скорости обучения для SGD в каждом измерении. Это расширение Adagrad и AdaDelta, которое стремится уменьшить агрессивную, монотонно убывающую скорость обучения.


### Основные особенности:

1. AdaSmooth вводит механизм динамического сглаживания, который позволяет улучшить управление изменением величины градиентов. Это помогает избежать резких колебаний и повышает стабильность обучения.

2. Использование моментных оценок градиентов с дополнительными механизмами, снижающими зависимость от гиперпараметров.

3. За счет более сбалансированного обновления весов AdaSmooth быстрее сходится к оптимальному решению по сравнению с Adam и RMSprop.

4. Механизм динамического сглаживания помогает бороться с переобучением, особенно при работе с шумными данными или небольшими выборками.

5. Благодаря адаптивному изменению скорости обучения оптимизатор хорошо подходит для задач, где встречаются разреженные градиенты.

### Основные этапы работы:

1. **Расчет градиентов**

   Как и в других адаптивных методах, сначала вычисляется градиент функции потерь g<sub>t</sub> на текущей итерации t<sub>t</sub>.

2. **Экспоненциальное скользящее среднее градиента** 

   AdaSmooth использует стандартное экспоненциальное усреднение градиента (первый момент):
```math
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
```	
где $\beta_1$ – коэффициент затухания момента.

3. **Адаптивное сглаживание второго момента** 

   Ключевая особенность AdaSmooth — динамическое сглаживание дисперсии градиента:
```math
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
```	
где $\beta_2$ – параметр сглаживания дисперсии.

4. **Динамическое обновление $\beta_2$**

    В отличие от фиксированного параметра в Adam, AdaSmooth обновляет β2β2​ с учетом текущей скорости изменения градиента. Это позволяет избежать резких скачков и делает процесс обучения более плавным.

5. **Коррекция смещения**

    Как и Adam, AdaSmooth учитывает необходимость коррекции начального смещения:
```math
\^m_t = \frac{m_t} {1 - \beta_1},  \^v_t =\frac{v_t}{ 1-\beta_2^t}
```	
6. **Обновление весов**

    Обновление параметров модели происходит по формуле:
```math
\theta_t = \theta_{t-1} -  \frac{\alpha}{\sqrt{\^v_t} + \epsilon}\^m_t
```    
где $\alpha$ – параметр сглаживания дисперсии.

### Преимущества AdamW:
- Быстрая сходимость.
- Уменьшенные колебания градиента.
- Снижение переобучения.
- Устойчивость к разреженным градиентам.

Таким образом, AdaSmooth можно рассматривать как улучшенный Adam, который адаптирует стратегию сглаживания для повышения стабильности и эффективности оптимизации.

---

# Использованные источники
- 
- https://arxiv.org/pdf/2204.00825v1