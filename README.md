# ПСИАПД
# Отчет по 1 лабораторной

## Оглавление

---
1. [Теоретическая база](#теоретическая-база)
2. Описание разработанной системы (алгоритмы, принципы работы, архитектура)
3. Результаты работы и тестирования системы (скриншоты, изображения, графики, закономерности)
4. Выводы по работе
5. [Использованные источники](#Использованные-источники)

## Теоретическая база
### Архитектура InceptionV3

**1. Введение**
**InceptionV3** — сверточная нейросеть для классификации изображений, представленная в статье *"Rethinking the Inception Architecture for Computer Vision" (Szegedy et al., 2016)*. Улучшает точность за счет факторизации сверток и оптимизации вычислений.

![InceptionV3 Архитектура](https://habrastorage.org/r/w1560/files/84d/8a8/0a2/84d8a80a299e440c9f1892485c2e0803.png)  

---

## Архитектура сети Inception v3 строится постепенно, шаг за шагом, как описано ниже:

1. Факторизованные свертки, что помогает снизить эффективность вычислений, поскольку уменьшает количество параметров, участвующих в сети. Он также контролирует эффективность сети.

2. Меньшие извилины: замена больших извилин меньшими определенно приводит к более быстрому обучению. Скажем, фильтр 5 × 5 имеет 25 параметров; два фильтра 3×3, заменяющие свертку 5×5, вместо этого имеют только 18 (3*3 + 3*3) параметров.

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/aUr-1L-cLAjz_rv6uOTxDD77ezXUJ9Brbf_4-wdKE-FztMeH-oYr2Y_-zV49i2Ty1dk33qOJuj8PSXiUvvJrCVA6dHeV1HW_Xb9iQyh5oPiUAd1Y-z01jCWp)

В середине мы видим свертку 3х3, а ниже — полносвязный слой. Поскольку обе свертки 3x3 могут разделять между собой веса, количество вычислений можно уменьшить.

3. Асимметричные свертки: Свертку 3 × 3 можно заменить сверткой 1 × 3, за которой следует свертка 3 × 1. Если свертку 3 × 3 заменить сверткой 2 × 2, количество параметров будет немного больше, чем предложенная асимметричная свертка.

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/image-8.png)

4. Вспомогательный классификатор: вспомогательный классификатор представляет собой небольшую CNN, вставленную между слоями во время обучения, и возникающие потери добавляются к основным потерям сети. В GoogLeNet вспомогательные классификаторы использовались для более глубокой сети, тогда как в Inception v3 вспомогательный классификатор действует как регуляризатор.

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/fGeYgLGnVzpmjMirbfVDkjpfwOX4wjCXEZc2YNmjrs8o24nzCmhO_OlejGpbEEzE9AlyZsJyc03iIfyHkSbvjbnAJMk1Q4cUqzy9KGCYyE8qyqq-Y8Z7xr_w)

5. Уменьшение размера сетки: Уменьшение размера сетки обычно выполняется путем объединения операций. Однако для борьбы с узкими местами вычислительных затрат предлагается более эффективный метод:

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/MCqdi4PPKaYV43T4mm0yS6tu1XqjM0wWfqtplEe7GSKkPBAQ0L8wZmGd9nVmYxSrn4uDAAO0WhqVjUUYEJGUiz68y-A52RsMWcy8Y58IIywrJbmcSFgwn7YU)
Все вышеперечисленные концепции объединены в окончательную архитектуру.
![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/jUYIO5_eq0hUmiBNbagOFb84C8Y9GxeedGUYNd-LIbhAlpW-1o8xSeNypMnbD6p-XsrAQvup3FeWXrAoZig7l7Y9WIK3uDHooEMEKiNNQ2qt0PfA4Zfsyltn)

## Обучение и результаты Inception v3
Inception v3 был обучен на ImageNet и сравнен с другими современными моделями, как показано ниже.

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/Y7GF27r8Iu-d0DmhAWKR_b_MCRP0mN9hx-oEGuxrlJup9W2NpoyofrcRgTEFG_DYamlNtXeEL5lHibW7N9NQXd4uP7ql-PLgtn1fmaJJ98bBTUEqxGYLf9bZ)

Как показано в таблице, при дополнении вспомогательным классификатором, факторизацией сверток, RMSProp и сглаживанием меток Inception v3 может достичь самого низкого уровня ошибок по сравнению со своими современниками.

## AdaSmooth

**AdaSmooth** — это метод стохастической оптимизации, который позволяет использовать метод скорости обучения для SGD в каждом измерении. Это расширение Adagrad и AdaDelta, которое стремится уменьшить агрессивную, монотонно убывающую скорость обучения.


### Основные особенности:

1. AdaSmooth вводит механизм динамического сглаживания, который позволяет улучшить управление изменением величины градиентов. Это помогает избежать резких колебаний и повышает стабильность обучения.

2. Использование моментных оценок градиентов с дополнительными механизмами, снижающими зависимость от гиперпараметров.

3. За счет более сбалансированного обновления весов AdaSmooth быстрее сходится к оптимальному решению по сравнению с Adam и RMSprop.

4. Механизм динамического сглаживания помогает бороться с переобучением, особенно при работе с шумными данными или небольшими выборками.

5. Благодаря адаптивному изменению скорости обучения оптимизатор хорошо подходит для задач, где встречаются разреженные градиенты.

### Основные этапы работы:

1. **Расчет градиентов**

   Как и в других адаптивных методах, сначала вычисляется градиент функции потерь g<sub>t</sub> на текущей итерации t<sub>t</sub>.

2. **Экспоненциальное скользящее среднее градиента** 

   AdaSmooth использует стандартное экспоненциальное усреднение градиента (первый момент):
```math
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
```	
где $\beta_1$ – коэффициент затухания момента.

3. **Адаптивное сглаживание второго момента** 

   Ключевая особенность AdaSmooth — динамическое сглаживание дисперсии градиента:
```math
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
```	
где $\beta_2$ – параметр сглаживания дисперсии.

4. **Динамическое обновление $\beta_2$**

    В отличие от фиксированного параметра в Adam, AdaSmooth обновляет β2β2​ с учетом текущей скорости изменения градиента. Это позволяет избежать резких скачков и делает процесс обучения более плавным.

5. **Коррекция смещения**

    Как и Adam, AdaSmooth учитывает необходимость коррекции начального смещения:
```math
\^m_t = \frac{m_t} {1 - \beta_1},  \^v_t =\frac{v_t}{ 1-\beta_2^t}
```	
6. **Обновление весов**

    Обновление параметров модели происходит по формуле:
```math
\theta_t = \theta_{t-1} -  \frac{\alpha}{\sqrt{\^v_t} + \epsilon}\^m_t
```    
где $\alpha$ – параметр сглаживания дисперсии.

### Преимущества AdamW:
- Быстрая сходимость.
- Уменьшенные колебания градиента.
- Снижение переобучения.
- Устойчивость к разреженным градиентам.

Таким образом, AdaSmooth можно рассматривать как улучшенный Adam, который адаптирует стратегию сглаживания для повышения стабильности и эффективности оптимизации.

---

## Использованные источники
- https://habr.com/ru/articles/302242/
- https://arxiv.org/pdf/2204.00825v1

---
подготовили:  
Куляев, Рухлова  
449м
