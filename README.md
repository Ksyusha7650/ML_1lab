# ПСИАПД
# Отчет по 1 лабораторной

## Оглавление

---
1. [Теоретическая база](#теоретическая-база)
2. [Описание разработанной системы](#Описание-разработанной-системы)
3. [Результаты работы и тестирования системы](#Результаты-работы-и-тестирования-системы)
4. [Выводы по работе](#Выводы-по-работе)
5. [Использованные источники](#Использованные-источники)

## Теоретическая база
### Свёрточные нейросети
**Свертка**
Свертка — это основной строительный блок CNN. Она заключается в применении фильтров (или ядер) к входным данным для выделения различных признаков, таких как края, текстуры и формы. Фильтр скользит по изображению и вычисляет свертку, создавая карту признаков. Этот процесс позволяет выделить важные детали изображения, которые затем используются для дальнейшей обработки.

**Активация**
После свертки применяется функция активации, чтобы добавить нелинейность в модель. Наиболее популярной функцией активации является ReLU (Rectified Linear Unit), которая заменяет все отрицательные значения на ноль. Это помогает модели лучше справляться с сложными задачами, так как нелинейные функции позволяют нейронной сети моделировать более сложные зависимости.

Функция активации ReLU имеет простую формулу: f(x) = max(0, x). Это означает, что все отрицательные значения заменяются на ноль, а положительные остаются без изменений. Такая простота делает ReLU очень эффективной и быстрой в вычислении.

**Пулинг**
Пулинг (или субдискретизация) уменьшает размер карты признаков, сохраняя важные признаки. Наиболее распространенным методом является максимальный пулинг (max pooling), который выбирает максимальное значение из каждого подмассива. Это помогает уменьшить количество параметров и вычислительных затрат, сохраняя при этом важные характеристики изображения.

###Архитектура типичной CNN: слои и их функции
**Входной слой**
Входной слой принимает изображение в виде массива чисел. Например, цветное изображение размером 32x32 пикселя будет представлено как массив размером 32x32x3 (где 3 — количество цветовых каналов: красный, зеленый, синий). Этот массив чисел является исходными данными, которые будут обработаны нейронной сетью.

**Сверточные слои**
Сверточные слои выполняют свертку с различными фильтрами для выделения признаков. Каждый сверточный слой создает несколько карт признаков, которые затем передаются на следующий слой. Эти слои являются основными компонентами CNN и выполняют основную работу по выделению признаков из изображения.

Каждый сверточный слой может использовать несколько фильтров, каждый из которых выделяет разные признаки. Например, один фильтр может выделять горизонтальные края, другой — вертикальные, а третий — диагональные. Эти карты признаков затем объединяются для создания более сложных представлений изображения.

**Слои активации**
Слои активации применяют функцию активации (например, ReLU) к картам признаков, добавляя нелинейность в модель. Это позволяет модели лучше справляться с сложными задачами, так как нелинейные функции позволяют нейронной сети моделировать более сложные зависимости.

**Пулинговые слои**
Пулинговые слои уменьшают размер карт признаков, сохраняя важные признаки и уменьшая вычислительные затраты. Это помогает модели быть более эффективной и быстрой, особенно при обработке больших изображений.

**Полносвязные слои**
Полносвязные слои (fully connected layers) соединяют все нейроны предыдущего слоя с каждым нейроном текущего слоя. Они используются для окончательной классификации или регрессии. Эти слои являются завершающими компонентами CNN и выполняют основную работу по принятию решений на основе выделенных признаков.

**Выходной слой**
Выходной слой генерирует окончательный результат, например, метку класса для задачи классификации. Это может быть вероятность принадлежности изображения к одному из нескольких классов или конкретное значение для задачи регрессии.

![image](https://github.com/user-attachments/assets/65e55b69-d159-4163-b82e-a8fa6275a5bc)


## Описание разработанной системы
### Архитектура InceptionV3

**1. Введение**
**InceptionV3** — сверточная нейросеть для классификации изображений, представленная в статье *"Rethinking the Inception Architecture for Computer Vision" (Szegedy et al., 2016)*. Улучшает точность за счет факторизации сверток и оптимизации вычислений.

![InceptionV3 Архитектура](https://habrastorage.org/r/w1560/files/84d/8a8/0a2/84d8a80a299e440c9f1892485c2e0803.png)  

---

### Архитектура сети Inception v3 строится постепенно, шаг за шагом, как описано ниже:

1. **Факторизованные свертки**, что помогает снизить эффективность вычислений, поскольку уменьшает количество параметров, участвующих в сети. Он также контролирует эффективность сети.

2. **Меньшие извилины**: замена больших извилин меньшими определенно приводит к более быстрому обучению. Скажем, фильтр 5 × 5 имеет 25 параметров; два фильтра 3×3, заменяющие свертку 5×5, вместо этого имеют только 18 (3*3 + 3*3) параметров.

![image](https://github.com/user-attachments/assets/1bc3f996-09fb-436a-90da-e591a2455f97)

В середине мы видим свертку 3х3, а ниже — полносвязный слой. Поскольку обе свертки 3x3 могут разделять между собой веса, количество вычислений можно уменьшить.

3. **Асимметричные свертки**: Свертку 3 × 3 можно заменить сверткой 1 × 3, за которой следует свертка 3 × 1. Если свертку 3 × 3 заменить сверткой 2 × 2, количество параметров будет немного больше, чем предложенная асимметричная свертка.

![](https://ru.linux-console.net/common-images/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/image-8.png)

4. **Вспомогательный классификатор**: вспомогательный классификатор представляет собой небольшую CNN, вставленную между слоями во время обучения, и возникающие потери добавляются к основным потерям сети. В GoogLeNet вспомогательные классификаторы использовались для более глубокой сети, тогда как в Inception v3 вспомогательный классификатор действует как регуляризатор.

![image](https://github.com/user-attachments/assets/b0e2ce2d-bd16-4c71-89cc-5cb25171c360)

5. **Уменьшение размера сетки**: Уменьшение размера сетки обычно выполняется путем объединения операций. Однако для борьбы с узкими местами вычислительных затрат предлагается более эффективный метод:

![image](https://github.com/user-attachments/assets/1163ec8d-97fe-40e7-800f-639fab8ba544)

Все вышеперечисленные концепции объединены в окончательную архитектуру.
![image](https://github.com/user-attachments/assets/b692b56f-369e-4d14-9203-8f773c595b6f)


### Обучение и результаты Inception v3
Inception v3 был обучен на ImageNet и сравнен с другими современными моделями, как показано ниже.

![image](https://github.com/user-attachments/assets/776cb93e-c883-433b-a221-6cd4768f4d19)


Как показано в таблице, при дополнении вспомогательным классификатором, факторизацией сверток, RMSProp и сглаживанием меток Inception v3 может достичь самого низкого уровня ошибок по сравнению со своими современниками.

## AdaSmooth

**AdaSmooth** — это метод стохастической оптимизации, который позволяет использовать метод скорости обучения для SGD в каждом измерении. Это расширение Adagrad и AdaDelta, которое стремится уменьшить агрессивную, монотонно убывающую скорость обучения.


### Основные особенности:

1. AdaSmooth вводит механизм динамического сглаживания, который позволяет улучшить управление изменением величины градиентов. Это помогает избежать резких колебаний и повышает стабильность обучения.

2. Использование моментных оценок градиентов с дополнительными механизмами, снижающими зависимость от гиперпараметров.

3. За счет более сбалансированного обновления весов AdaSmooth быстрее сходится к оптимальному решению по сравнению с Adam и RMSprop.

4. Механизм динамического сглаживания помогает бороться с переобучением, особенно при работе с шумными данными или небольшими выборками.

5. Благодаря адаптивному изменению скорости обучения оптимизатор хорошо подходит для задач, где встречаются разреженные градиенты.

### Основные этапы работы:

1. **Расчет градиентов**

   Как и в других адаптивных методах, сначала вычисляется градиент функции потерь g<sub>t</sub> на текущей итерации t<sub>t</sub>.

2. **Экспоненциальное скользящее среднее градиента** 

   AdaSmooth использует стандартное экспоненциальное усреднение градиента (первый момент):
```math
m_t = \beta_1m_{t-1} + (1-\beta_1)g_t
```	
где $\beta_1$ – коэффициент затухания момента.

3. **Адаптивное сглаживание второго момента** 

   Ключевая особенность AdaSmooth — динамическое сглаживание дисперсии градиента:
```math
v_t = \beta_2v_{t-1} + (1-\beta_2)g_t^2
```	
где $\beta_2$ – параметр сглаживания дисперсии.

4. **Динамическое обновление $\beta_2$**

    В отличие от фиксированного параметра в Adam, AdaSmooth обновляет β2β2​ с учетом текущей скорости изменения градиента. Это позволяет избежать резких скачков и делает процесс обучения более плавным.

5. **Коррекция смещения**

    Как и Adam, AdaSmooth учитывает необходимость коррекции начального смещения:
```math
\^m_t = \frac{m_t} {1 - \beta_1},  \^v_t =\frac{v_t}{ 1-\beta_2^t}
```	
6. **Обновление весов**

    Обновление параметров модели происходит по формуле:
```math
\theta_t = \theta_{t-1} -  \frac{\alpha}{\sqrt{\^v_t} + \epsilon}\^m_t
```    
где $\alpha$ – параметр сглаживания дисперсии.

### Преимущества AdamW:
- Быстрая сходимость.
- Уменьшенные колебания градиента.
- Снижение переобучения.
- Устойчивость к разреженным градиентам.

Таким образом, AdaSmooth можно рассматривать как улучшенный Adam, который адаптирует стратегию сглаживания для повышения стабильности и эффективности оптимизации.

## Результаты работы и тестирования системы

После запуска разработанных скриптов получились следующие результаты:

Таблица Train-метрик для оптимизаторов Adam и AdaSmooth
| Epoch | Adam Loss | Adam Acc\@1 | Adam Acc\@5 | AdaSmooth Loss | AdaSmooth Acc\@1 | AdaSmooth Acc\@5 |
| :---: | :-------: | :---------: | :---------: | :------------: | :--------------: | :--------------: |
|   1   |    9.25   |     0.69    |     2.53    |      22.72     |       0.76       |       2.57       |
|   2   |    6.92   |     0.62    |     2.67    |      8.49      |       0.63       |       2.85       |
|   3   |    6.91   |     0.61    |     2.59    |      8.49      |       0.76       |       2.89       |
|   4   |    6.91   |     0.52    |     2.32    |      8.47      |       0.64       |       2.95       |
|   5   |    6.90   |     0.47    |     2.29    |      8.45      |       0.99       |       3.53       |
|   6   |    6.89   |     0.62    |     2.52    |      8.42      |       0.76       |       3.66       |
|   7   |    6.87   |     0.66    |     2.77    |      8.41      |       1.03       |       3.87       |
|   8   |    6.85   |     0.66    |     2.39    |      8.39      |       1.04       |       4.24       |
|   9   |    6.81   |     0.88    |     3.45    |      6.25      |       0.74       |       3.23       |
|   10  |    6.76   |     1.19    |     4.81    |      6.76      |       1.19       |       4.81       |
|   11  |    6.72   |     1.19    |     4.74    |      6.72      |       1.19       |       4.74       |
|   12  |    6.70   |     1.46    |     5.39    |      6.70      |       1.46       |       5.39       |
|   13  |    6.65   |     2.34    |     6.25    |      6.84      |       0.67       |       3.37       |
|   14  |    6.87   |     0.82    |     2.71    |      6.87      |       0.79       |       2.72       |
|   15  |    6.87   |     0.76    |     2.67    |      6.87      |       0.76       |       2.67       |
|   16  |    6.86   |     0.71    |     2.62    |      6.86      |       0.71       |       2.62       |
|   17  |    6.82   |     0.89    |     3.50    |      6.82      |       0.89       |       3.50       |
|   18  |    6.80   |     1.08    |     3.35    |      6.80      |       1.08       |       3.35       |


Таблица Valid-метрик для оптимизаторов Adam и AdaSmooth
| Epoch | Adam Loss | Adam Acc\@1 | Adam Acc\@5 | AdaSmooth Loss | AdaSmooth Acc\@1 | AdaSmooth Acc\@5 |
| :---: | :-------: | :---------: | :---------: | :------------: | :--------------: | :--------------: |
|   1   |    5.33   |     0.52    |     2.80    |      5.31      |       0.53       |       2.63       |
|   2   |    5.30   |     0.81    |     2.96    |      5.31      |       0.53       |       3.06       |
|   3   |    5.29   |     0.83    |     3.08    |      6.15      |       0.55       |       2.71       |
|   4   |    8.59   |     0.54    |     2.90    |      5.37      |       0.82       |       3.02       |
|   5   |    5.32   |     0.56    |     2.62    |      5.29      |       0.54       |       3.07       |
|   6   |    5.34   |     0.58    |     3.06    |      5.34      |       0.58       |       3.06       |
|   7   |    5.31   |     0.55    |     3.14    |      5.31      |       0.55       |       3.14       |
|   8   |    5.31   |     0.55    |     2.97    |      5.30      |       0.72       |       2.71       |
|   9   |    5.29   |     0.55    |     2.76    |      5.29      |       0.83       |       3.08       |
|   10  |    5.60   |     0.79    |     3.23    |      5.60      |       0.79       |       3.23       |
|   11  |    5.29   |     0.80    |     3.06    |      5.29      |       0.80       |       3.06       |
|   12  |    5.28   |     0.83    |     3.05    |      5.39      |       0.81       |       3.22       |
|   13  |    5.30   |     0.55    |     2.96    |      5.30      |       0.56       |       2.92       |
|   14  |    5.29   |     0.54    |     2.71    |      5.29      |       0.53       |       2.71       |
|   15  |    5.29   |     0.54    |     2.71    |      5.32      |       0.56       |       2.62       |
|   16  |    5.30   |     0.52    |     2.73    |      5.59      |       0.63       |       2.92       |
|   17  |    5.30   |     0.56    |     2.97    |      5.28      |       0.83       |       3.05       |
|   18  |    5.30   |     0.56    |     2.97    |      5.29      |       0.82       |       3.02       |


![image](https://github.com/user-attachments/assets/b4318dfc-9f5f-4bbc-b096-cd20f4a4394d)

![image](https://github.com/user-attachments/assets/e9cc6a46-ce36-44c3-b2f5-aa86786b4d5e)

![image](https://github.com/user-attachments/assets/6deef6ce-28fb-4336-aae5-2ab318041c27)

![image](https://github.com/user-attachments/assets/28ab18bc-44b7-42f6-9adf-2d48e904a10c)

![image](https://github.com/user-attachments/assets/db0e164b-ed15-4324-add2-e3a496ff36da)


## Выводы по работе
В результате выполнения лабораторной работы была реализована нейронная сеть InceptionV3 с двумя вариантами оптимизатора:
- AdamW
- AdaSmooth

По итогам валидации модели показали следующие результаты Top‑1 Accuracy:
- Точность модели с оптимизатором AdamW составила 56 %.
- Точность модели с оптимизатором AdaSmooth составила 82 %.

Таким образом, использование адаптивного сглаженного оптимизатора AdaSmooth позволило существенно повысить качество обучения по сравнению с классическим AdamW.
---

## Использованные источники
- https://sky.pro/wiki/python/svertochnye-nejronnye-seti-cnn-chto-eto-i-kak-oni-rabotayut/
- https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti
- https://habr.com/ru/articles/302242/
- https://ru.linux-console.net/?p=34027
- https://arxiv.org/pdf/2204.00825v1

---
подготовили:  
Куляев, Рухлова  
449м
